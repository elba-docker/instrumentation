"""
Script to parse modified Moby (docker engine) container stat logs
"""

__author__ = "Joseph Azevedo"
__version__ = "1.0"

import numpy as np
import pandas as pd
from dateutil import parser
from collections import OrderedDict
import glob, os, json, csv, argparse


DESCRIPTION = "Script to parse modified Moby (docker engine) container stat logs"


class DataClass:
    def __getattr__(self, attr):
        try:
            return getattr(self, f"_{attr}")
        except AttributeError:
            pass


class NetworkEntry(DataClass):
    """
    Network stats from Moby log entries
    """

    def __init__(self, str):
        rx, tx = str.split("|")
        self._rx = NetworkDirectionStats(rx)
        self._tx = NetworkDirectionStats(tx)


class NetworkDirectionStats(DataClass):
    """
    Network stats from Moby log entries for a single direction (tx/rx)
    """

    def __init__(self, raw):
        num_bytes, packets, errors, dropped = raw.split(" ")
        self._bytes = int(num_bytes)
        self._packets = int(packets)
        self._errors = int(errors)
        self._dropped = int(dropped)


class BlkioEntry(DataClass):
    """
    Blkio Linux stats from Moby log entries
    """

    def __init__(self, raw):
        major, minor, value, op = raw.split(" ")
        self._major = int(major)
        self._minor = int(minor)
        self._value = int(value)
        self._op = op


class CpuUsage(DataClass):
    """
    CPUUsage Linux stats from Moby log entries
    """

    def __init__(self, row):
        self._total = int(row["cpu_stats.cpu_usage.total_usage"])
        self._percpu = [int(cpu.strip()) for cpu in
            row["cpu_stats.cpu_usage.percpu_usage"].split(',')]
        self._kernel = int(row["cpu_stats.cpu_usage.usage_in_kernelmode"])
        self._user = int(row["cpu_stats.cpu_usage.usage_in_usermode"])
        self._system_usage = int(row["cpu_stats.system_cpu_usage"])
        self._online_cpus = int(row["cpu_stats.online_cpus"])
        self._throttling_periods = int(row["cpu_stats.throttling_data.periods"])
        self._throttled_periods = int(row["cpu_stats.throttling_data.throttled_periods"])
        self._throttled_time = int(row["cpu_stats.throttling_data.throttled_time"])


class Memory(DataClass):
    """
    Memory Linux stats from Moby log entries
    """

    def __init__(self, row):
        self._usage = int(row["memory_stats.usage"])
        self._max_usage = int(row["memory_stats.max_usage"])
        self._stats = json.loads(row["memory_stats.stats"])
        self._failcnt = int(row["memory_stats.failcnt"])
        self._limit = int(row["memory_stats.limit"])


class Blkio(DataClass):
    """
    Blkio Linux stats from Moby log entries
    """

    def __init__(self, row):
        self._service_bytes = parse_blkio(row["blkio_stats.io_service_bytes_recursive"])
        self._serviced = parse_blkio(row["blkio_stats.io_serviced_recursive"])
        self._queue = parse_blkio(row["blkio_stats.io_queue_recursive"])
        self._service_time = parse_blkio(row["blkio_stats.io_service_time_recursive"])
        self._wait_time = parse_blkio(row["blkio_stats.io_wait_time_recursive"])
        self._merged = parse_blkio(row["blkio_stats.io_merged_recursive"])
        self._time = parse_blkio(row["blkio_stats.io_time_recursive"])
        self._sectors = parse_blkio(row["blkio_stats.sectors_recursive"])


class PidsStats(DataClass):
    """
    Pids Linux stats from Moby log entries
    """

    def __init__(self, row):
        self._current = int(row["pids_stats.current"])
        self._limit= int(row["pids_stats.limit"])


class LogEntry(DataClass):
    """
    A modified Docker engine (Moby) log entry, generated by the
    stats collector in CSV mode
    """

    def __init__(self, row, entries=None):
        """
        Initializes and parses a LogEntry
        Note: discards all statistics that are only populated by moby on Windows
        """

        self._read = int(row["read"])
        self._preread = int(row["preread"])
        self._name = row["name"]
        self._id = row["id"]
        self._cpu = CpuUsage(row)
        self._memory = Memory(row)
        self._pids = PidsStats(row)
        self._blkio = Blkio(row)
        self._networks = {key: NetworkEntry(entry) for key, entry in
            json.loads(row["networks"]).items()}

        # Try to resolve pre-read statistics
        self._pre = None
        if entries is not None:
            if self._preread in entries:
                pre_entry = entries[self._preread]
                if pre_entry is not None:
                    self._pre = pre_entry


def parse_blkio(raw):
    """
    Parses a Blkio entry array
    """

    split = [entry.strip() for entry in raw.split(",")]
    return [BlkioEntry(entry) for entry in split if len(entry.split(" ")) == 4]


def main(root=None):
    """
    Parses and processes all files in the directory
    """

    resolved_root = os.path.abspath(root) if root is not None else os.getcwd()
    files = get_all_files(resolved_root, "log")
    data = parse_all(files)
    for (file, entries) in data.items():
        analyze_timestamps(entries)


def parse_all(files):   
    """
    Parses all of the given CSV filenames
    """

    parsed = {}
    for file in files:
        relative = os.path.relpath(file)
        print(f"Parsing log data from {relative}... ", end="", flush=True)
        log_data = load_file(file)
        print("done")
        parsed[file] = log_data
    return parsed


def analyze_timestamps(entries):
    """
    Analyzes timestamps to verify correct collection interval
    """

    deltas = get_ts_deltas(entries)
    deltas_df = pd.DataFrame({'Read deltas (ms)': deltas})
    print(deltas_df.describe(include='all'))


def get_ts_deltas(entries):
    """
    Gets timestamp deltas
    """

    return find_deltas([float(timestamp) / 1E6 for timestamp in entries])


def remove_outliers(arr, z=1.5):
    """
    Removes outliers (classified as outside [q25 - 1.5*iqr, q75 + 1.5*iqr])
    """

    q25, q75 = np.percentile(arr, 25), np.percentile(arr, 75)
    iqr = q75 - q25
    limit = z * iqr
    return [a for a in arr if a >= q25 - limit and a <= q75 + limit]


def find_deltas(arr):
    """
    Creates a new array that is the differences between consecutive elements in
    a numeric series. The new array is len(arr) - 1
    """

    return [j-i for i, j in zip(arr[:-1], arr[1:])]


def load_file(filename):
    """
    Loads an output file from modified moby into an ordered dictionary
    of read timestamp (int) -> LogEntry in the order of logging
    """

    entries = OrderedDict()
    with open(filename, "r") as csvfile:
        csv_reader = csv.DictReader(csvfile) 

        # skip header row
        next(csv_reader)

        for row in csv_reader:
            entry = LogEntry(dict(row))
            entries[entry.read] = entry
    return entries


def get_all_files(dir, ext):
    """
    Gets all files in the directory with the given extension
    """

    extension_suffix = f".{ext}"
    files = []
    for file in os.listdir(dir):
        if file.endswith(extension_suffix):
            files.append(os.path.join(dir, file))
    return files


def bootstrap():
    """
    Runs CLI parsing/execution
    """

    # Argument definitions
    parser = argparse.ArgumentParser(description=DESCRIPTION)
    parser.add_argument("--root", "-r", metavar="path",
                        help="the path to find log files in (defaults to current directory)")

    # Parse arguments
    parsed_args = parser.parse_args()
    main(root=parsed_args.root)


if __name__ == "__main__":
    bootstrap()
