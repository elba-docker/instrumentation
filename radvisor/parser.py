"""
Script to parse rAdvisor container stat logs
"""

__author__ = "Joseph Azevedo"
__version__ = "1.0"

import numpy as np
import pandas as pd
from dateutil import parser
from collections import OrderedDict
from more_itertools import peekable
import glob, os, json, csv, argparse
from csv import Error


DESCRIPTION = "Script to parse rAdvisor container stat logs"


class DataClass:
    def __getattr__(self, attr):
        try:
            return getattr(self, f"_{attr}")
        except AttributeError:
            pass


# class NetworkEntry(DataClass):
#     """
#     Network stats from rAdvisor log entries
#     """

#     def __init__(self, str):
#         rx, tx = str.split("|")
#         self._rx = NetworkDirectionStats(rx)
#         self._tx = NetworkDirectionStats(tx)


# class NetworkDirectionStats(DataClass):
#     """
#     Network stats from rAdvisor log entries for a single direction (tx/rx)
#     """

#     def __init__(self, raw):
#         num_bytes, packets, errors, dropped = raw.split(" ")
#         self._bytes = int(num_bytes)
#         self._packets = int(packets)
#         self._errors = int(errors)
#         self._dropped = int(dropped)


# class BlkioEntry(DataClass):
#     """
#     Blkio Linux stats from rAdvisor log entries
#     """

#     def __init__(self, raw):
#         major, minor, value, op = raw.split(" ")
#         self._major = int(major)
#         self._minor = int(minor)
#         self._value = int(value)
#         self._op = op


class CpuUsage(DataClass):
    """
    CPU Usage stats from rAdvisor log entries
    """

    def __init__(self, row):
        self._total = int(row["cpu.usage.total"])
        self._percpu = [int(cpu.strip()) for cpu in
            row["cpu.usage.percpu"].split(' ')]
        self._system_usage = int(row["cpu.usage.system"])
        self._user_usage = int(row["cpu.usage.user"])
        self._system_stat = int(row["cpuacct.stat.system"])
        self._user_stat = int(row["cpuacct.stat.user"])
        self._throttling_periods = int(row["cpu.throttling.periods"])
        self._throttled_periods = int(row["cpu.throttling.throttled"])
        self._throttled_time = int(row["cpu.throttling.throttled_time"])


# class Memory(DataClass):
#     """
#     Memory Linux stats from rAdvisor log entries
#     """

#     def __init__(self, row):
#         self._usage = int(row["memory_stats.usage"])
#         self._max_usage = int(row["memory_stats.max_usage"])
#         self._stats = json.loads(row["memory_stats.stats"])
#         self._failcnt = int(row["memory_stats.failcnt"])
#         self._limit = int(row["memory_stats.limit"])


# class Blkio(DataClass):
#     """
#     Blkio Linux stats from rAdvisor log entries
#     """

#     def __init__(self, row):
#         self._service_bytes = parse_blkio(row["blkio_stats.io_service_bytes_recursive"])
#         self._serviced = parse_blkio(row["blkio_stats.io_serviced_recursive"])
#         self._queue = parse_blkio(row["blkio_stats.io_queue_recursive"])
#         self._service_time = parse_blkio(row["blkio_stats.io_service_time_recursive"])
#         self._wait_time = parse_blkio(row["blkio_stats.io_wait_time_recursive"])
#         self._merged = parse_blkio(row["blkio_stats.io_merged_recursive"])
#         self._time = parse_blkio(row["blkio_stats.io_time_recursive"])
#         self._sectors = parse_blkio(row["blkio_stats.sectors_recursive"])


class PidsStats(DataClass):
    """
    Pids Linux stats from rAdvisor log entries
    """

    def __init__(self, row):
        self._current = int(row["pids.current"])
        self._max= 0 if row["pids.max"] == "max" else int(row["pids.max"])


class LogEntry(DataClass):
    """
    A rAdvisor log entry, generated by the tats collector in CSV mode
    """

    def __init__(self, row, preread=None, entries=None):
        """
        Initializes and parses a LogEntry
        """

        self._read = int(row["read"])
        # self._preread = int(row["preread"])
        # self._name = row["name"]
        # self._id = row["id"]
        self._cpu = CpuUsage(row)
        # self._memory = Memory(row)
        self._pids = PidsStats(row)
        # self._blkio = Blkio(row)
        # self._networks = {key: NetworkEntry(entry) for key, entry in
        #     json.loads(row["networks"]).items()}

        # Try to resolve pre-read statistics
        self._pre = None
        if entries is not None:
            if preread in entries:
                pre_entry = entries[preread]
                if pre_entry is not None:
                    self._pre = pre_entry


# def parse_blkio(raw):
#     """
#     Parses a Blkio entry array
#     """

#     split = [entry.strip() for entry in raw.split(",")]
#     return [BlkioEntry(entry) for entry in split if len(entry.split(" ")) == 4]


def main(root=None):
    """
    Parses and processes all files in the directory
    """

    resolved_root = os.path.abspath(root) if root is not None else os.getcwd()
    files = get_all_files(resolved_root, "log")
    data = parse_all(files)
    for (_, entries) in data.items():
        analyze_timestamps(entries)


def parse_all(files):   
    """
    Parses all of the given CSV filenames
    """

    parsed = {}
    for file in files:
        relative = os.path.relpath(file)
        print(f"Parsing log data from {relative}... ", end="", flush=True)
        log_data = load_file(file)
        print("done")
        parsed[file] = log_data
    return parsed


def analyze_timestamps(entries):
    """
    Analyzes timestamps to verify correct collection interval
    """

    deltas = get_ts_deltas(entries)
    deltas_df = pd.DataFrame({'Read deltas (ms)': deltas})
    print(deltas_df.describe(include='all'))


def get_ts_deltas(entries):
    """
    Gets timestamp deltas
    """

    return find_deltas([float(timestamp) / 1E6 for timestamp in entries])


def remove_outliers(arr, z=1.5):
    """
    Removes outliers (classified as outside [q25 - 1.5*iqr, q75 + 1.5*iqr])
    """

    q25, q75 = np.percentile(arr, 25), np.percentile(arr, 75)
    iqr = q75 - q25
    limit = z * iqr
    return [a for a in arr if a >= q25 - limit and a <= q75 + limit]


def find_deltas(arr):
    """
    Creates a new array that is the differences between consecutive elements in
    a numeric series. The new array is len(arr) - 1
    """

    return [j-i for i, j in zip(arr[:-1], arr[1:])]


def load_file(filename):
    """
    Loads an output file from rAdvisor into an ordered dictionary
    of read timestamp (int) -> LogEntry in the order of logging
    """

    entries = OrderedDict()
    with open(filename, "r") as csvfile:
        # skip all lines until the first line of the header
        file_iter = peekable(csvfile)
        while not file_iter.peek().startswith("read"):
            next(file_iter)

        csv_reader = csv.DictReader(file_iter) 

        # skip header row
        next(csv_reader)

        preread = None
        try:
            for row in csv_reader:
                entry = LogEntry(dict(row), entries=entries, preread=preread)
                entries[entry.read] = entry
                preread = entry.read
        except Error as e:
            print(e)
            print("An error ocurred. continuing...\n")
    return entries


def get_all_files(dir, ext):
    """
    Gets all files in the directory with the given extension
    """

    extension_suffix = f".{ext}"
    files = []
    for file in os.listdir(dir):
        if file.endswith(extension_suffix):
            files.append(os.path.join(dir, file))
    return files


def bootstrap():
    """
    Runs CLI parsing/execution
    """

    # Argument definitions
    parser = argparse.ArgumentParser(description=DESCRIPTION)
    parser.add_argument("--root", "-r", metavar="path",
                        help="the path to find log files in (defaults to current directory)")

    # Parse arguments
    parsed_args = parser.parse_args()
    main(root=parsed_args.root)


if __name__ == "__main__":
    bootstrap()
